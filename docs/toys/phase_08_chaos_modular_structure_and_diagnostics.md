# Phase 8 — Chaos, Modular Structure, & Advanced Diagnostics

**Toys 071–080**

---

## Toy 071 — Analytic Continuation as an Ill-Posed Inverse Problem

This toy constructs a controlled example of analytic continuation by starting from a chosen nonnegative spectral density and computing a Euclidean-time correlator sampled on a finite grid. The physical setup mirrors a common situation in quantum field theory where one measures or computes an imaginary-time correlator and wishes to infer real-time information encoded in a spectral function. The quantity of interest is the spectral density $\rho(\omega)$, related to the Euclidean correlator by the Laplace-type relation $G_E(t)=\int_0^\infty d\omega\,\rho(\omega)e^{-\omega t}$, evaluated here by explicit quadrature.

The purpose of the toy is not to predict physical spectra but to expose a fundamental limitation: recovering $\rho(\omega)$ from finitely sampled $G_E(t)$ is an ill-posed inverse problem. The reconstruction is framed as a Tikhonov-regularized least-squares problem, $\rho_\lambda=\arg\min\|K\rho-G\|^2+\lambda\|\rho\|^2$, where different regularization strengths $\lambda$ stabilize the inversion in different ways. The toy demonstrates that even when multiple choices of $\lambda$ yield nearly indistinguishable Euclidean fits, the inferred spectra can differ substantially, reflecting a stress test of analytic continuation rather than a statement about the true underlying physics.

The JSON output should be read with this limitation in mind. Small Euclidean residuals and tiny differences between reconstructed $G_E(t)$ values indicate that the Euclidean data are well matched, while large condition numbers and sizable $\|\rho_{\lambda_a}-\rho_{\lambda_b}\|$ highlight non-uniqueness in the spectral reconstruction. The real-time proxy values, based on $G_{\mathrm{rt}}(t)\approx\int d\omega\,\rho(\omega)\sin(\omega t)$, amplify these differences and illustrate sensitivity to high-frequency structure. These numbers should not be over-interpreted as physical predictions; their role is to connect the numerical trends back to the integral relations and to show how apparently benign Euclidean agreement can mask radically different implied real-time behavior.

---

## Toy 072 — Regulator Dependence of OTOC-Based Chaos Diagnostics

This toy studies operator growth using an out-of-time-ordered commutator (OTOC) in a finite-dimensional quantum system, implemented as a single large spin evolving under kicked-top–like Floquet dynamics. The physical setup replaces an infinite-dimensional Hilbert space with a spin-$J$ representation of SU(2), where the dimension $D=2J+1$ acts as a UV cutoff. The quantity of interest is the OTOC defined here as $C(t)=-\mathrm{Tr}([W(t),V]^2)/D$, with $W=J_z$ and $V=J_x$, which serves as a proxy for operator growth and quantum chaos.

The toy exists to illustrate that commonly used chaos diagnostics are not regulator-independent objects. In truncated or finite systems, early-time behavior may appear to show rapid growth suggestive of chaotic dynamics, but the extracted growth rates, saturation values, and even qualitative features depend on the cutoff $J$. This setup therefore tests a conceptual failure mode: interpreting OTOCs as universal indicators of chaos without accounting for how the UV regularization and operator definitions enter their construction, rather than making any claim about true Lyapunov exponents or universal bounds.

The JSON results should be interpreted comparatively across different values of $J$. The per-cutoff summaries report the Hilbert space dimension, a fitted early-time growth rate when meaningful, the saturation value of the OTOC, and the time at which that saturation occurs. Trends to notice are how the magnitude of the OTOC and its saturation scale grow rapidly with increasing $J$, while fitted growth rates may be unstable or undefined over the same nominal time window. Individual time-series entries show large negative values whose absolute size depends strongly on the cutoff; these should not be over-interpreted as physical observables, but rather connected back to the definition of $C(t)$ and the finite-dimensional trace normalization that makes the regulator dependence explicit.

---

## Toy 073 — Entanglement Entropy Divergence Under a Lattice Cutoff

This toy computes ground-state entanglement entropy in a discretized 1D free scalar field, modeled as a periodic harmonic chain with lattice spacing $a$, total physical length $L_{\mathrm{tot}}$, and a contiguous subsystem of fixed physical size $L_{\mathrm{phys}}$. The physical setup is the vacuum of a quadratic Hamiltonian, so the state is Gaussian and is fully characterized by position and momentum covariances restricted to the block. The quantity of interest is the von Neumann entropy $S$ of the block, extracted from symplectic eigenvalues $\nu_k$ of $\sqrt{X_A P_A}$ via $S=\sum_k[(\nu_k+1/2)\ln(\nu_k+1/2)-(\nu_k-1/2)\ln(\nu_k-1/2)]$.

The toy exists to demonstrate that vacuum entanglement is intrinsically UV sensitive: as the cutoff is pushed to shorter distances, additional short-range correlations across the entangling cut contribute, and $S$ grows without bound in the continuum limit. In 1+1D this is often summarized as a logarithmic divergence with the regulator, schematically $S\sim A+B\ln(L_{\mathrm{phys}}/a)$, but the constants and even the clean visibility of the scaling depend on the precise regulator and model details (here, a lattice with a mass parameter). The point is a stress test of “entanglement as an observable” under regularization changes, not a claim that any single lattice value is a physical prediction.

To interpret the JSON, treat each `sample_points` entry as one cutoff choice: it reports $a$, the induced site counts $N\approx L_{\mathrm{tot}}/a$ and $n_A\approx L_{\mathrm{phys}}/a$, the entropy `entanglement_entropy_S`, and the corresponding `ln_L_over_a` used to visualize the expected logarithmic trend. The key signal is monotonic growth of $S$ as $a$ decreases at fixed $L_{\mathrm{phys}}$, and the linearity of $S$ versus $\ln(L_{\mathrm{phys}}/a)$ summarized by the fit parameters `A_intercept`, `B_slope`, and `R2` (here showing a near-perfect log fit). What should not be over-interpreted are the absolute entropy values or the fitted intercept as universal numbers; they are regulator- and parameter-dependent, and their role is to connect the observed scaling back to the covariance-based definition of $\nu_k$ and the expected logarithmic UV sensitivity.

---

## Toy 074 — Constraint Violation Under Effective Evolution

This toy models constraint propagation using a minimal finite-dimensional proxy: two coupled harmonic oscillators representing longitudinal and transverse modes evolving in time. The physical setup imposes a linear constraint linking the coordinates, intended to mimic a Gauss-law–type condition that selects the physical subspace. The quantity of interest is the constraint value itself, defined as $C(t)=q_L(t)+\alpha q_T(t)$, which should remain zero if the dynamics exactly preserves the constraint.

The toy exists to illustrate a failure mode of effective or truncated evolution schemes. While the exact equations of motion are constructed to preserve the constraint for all times when it is satisfied initially, the effective evolution deliberately drops a coupling term, breaking constraint propagation. This demonstrates that even when initial data lies exactly in the physical subspace, approximate dynamics can drive the system away from it, exposing a conceptual limitation analogous to constraint-violation growth in approximate treatments of gauge theories or general relativity, rather than making any physical prediction.

The JSON output should be read by comparing exact and effective evolutions over time. The `constraint_exact` field remains identically zero, confirming perfect preservation, while `constraint_effective` departs smoothly from zero and grows in magnitude, indicating leakage out of the constrained subspace. The accompanying `energy_exact` and `energy_effective` entries show mild energy drift under effective evolution, which correlates with constraint violation but should not be over-interpreted as a stability statement. The important signal is the systematic growth of $|C(t)|$ under effective dynamics, tying the numerical trend back to the defining constraint equation and highlighting how approximate evolution breaks its propagation.

---

## Toy 075 — Linearization Instability via a Jacobi Integrability Obstruction

This toy uses a tiny algebraic model of “theory data” to mimic linearization instability: it treats the structure constants $f_{ab}{}^{c}$ of a Lie algebra as the object being deformed and checks whether the deformation remains consistent. The physical setup is a stand-in for symmetry/current algebras in QFT: start from $\mathfrak{su}(2)$ with brackets fixed by the Levi–Civita tensor and consider a perturbed bracket $f\to f+\epsilon\,\Delta f$. The quantity of interest is the Jacobi identity residual, i.e. how far $[T_a,[T_b,T_c]]+\text{cyc}$ deviates from zero for the deformed algebra.

The toy exists to expose the gap between “solves the linearized constraints” and “comes from any exact consistent theory.” At first order in $\epsilon$, the Jacobi identity linearizes to a constraint on $\Delta f$ (a proxy for linearized consistency conditions), but exact consistency requires that higher-order terms also vanish, which can impose an integrability/obstruction condition. The stress test is that one can arrange for the linearized Jacobi constraint to look small while the full Jacobi identity is nonetheless violated for any $\epsilon\neq 0$, so the deformation is not integrable into an exact Lie algebra even though it appears acceptable in linear theory.

To interpret the JSON, read each `sample_points` entry as one deformation size $\epsilon$ with three key numbers: `linearized_residual_proxy`, `jacobi_residual_full`, and `jacobi_residual_over_eps2`. The important trend is that `jacobi_residual_full` stays nonzero for all sampled $\epsilon$ and scales approximately like $\epsilon^2$ at small $\epsilon$, which is why `jacobi_residual_over_eps2` approaches a roughly constant “obstruction coefficient” summarized in `estimated_second_order_obstruction_coeff`. What should not be over-interpreted are the absolute magnitudes of these coefficients as universal physics; they are artifacts of the chosen deformation and norm, and their role is to connect the observed scaling back to the idea that second-order consistency conditions can obstruct deformations even when first-order (linearized) checks appear acceptable.

---

## Toy 076 — Partial-Wave Unitarity as a Cutoff Stress Test

This toy builds a minimal 2→2 scattering proxy where the amplitude grows with center-of-mass energy and is converted into a partial-wave diagnostic. The physical setup is an EFT-like expansion in the Mandelstam variable $s$, with a simple polynomial amplitude and an $s$-wave projection proxy $a_0(s)=A(s)/(16\pi)$. The quantity of interest is whether the real partial-wave amplitude stays within the elastic unitarity bound $|\mathrm{Re}\,a_0(s)|\le 1/2$ as $s$ increases.

The toy exists to demonstrate how naive hard cutoffs or inconsistent truncations can break unitarity even when the low-energy expansion looks innocuous. It implements a “cutoff mismatch” by modifying the amplitude above a cutoff scale $\Lambda_{\mathrm{cut}}$ with a step-like factor, intended as a proxy for clipping internal states without consistently adjusting the full dynamics. This is not a physical model of loops or the optical theorem; it is a stress test showing that regulator choices can introduce discontinuous or enhanced high-energy behavior that would be flagged immediately by partial-wave bounds.

To interpret the JSON, use `sample_points` to compare `a0_uncut` and `a0_cut` at each scanned $s$, and focus on the derived `violation_uncut` and `violation_cut` fields, which measure by how much $|a_0|$ exceeds $1/2$. In this run, both `first_s_violation_uncut` and `first_s_violation_cut` are null and `max_violation_cut_over_samples` is 0.0, meaning the scanned $s$ range never reaches the bound even after the cutoff-induced jump (visible as a larger `a0_cut` for $s>\Lambda_{\mathrm{cut}}^2$). What should not be over-interpreted is the absence of violation as a statement of “unitarity restoration”; it only reflects that, for the chosen parameters and sampled energies, $a_0(s)=A(s)/(16\pi)$ remains far below $1/2$, so the diagnostic does not yet enter the regime where the bound becomes constraining.

---

## Toy 077 — Nonlocality of the Modular Hamiltonian in Generic Regions

This toy computes the modular Hamiltonian for the reduced ground state of a free fermion chain restricted to a finite region $A$. The physical setup is a 1D tight-binding model in its zero-temperature ground state, where the reduced density matrix $\rho_A$ is fixed entirely by the correlation matrix $C_A$ on the sites in $A$. The quantity of interest is the quadratic modular kernel $h_{ij}$ defined by $h=\log((I-C_A)/C_A)$ in the single-particle basis, so that $K_A=\sum_{ij\in A} h_{ij} c_i^\dagger c_j$ generates the “thermal” form $\rho_A\propto e^{-K_A}$.

The toy exists to illustrate a limitation of “local thermality” intuition. While $\rho_A$ is always thermal with respect to $K_A$ by definition, $K_A$ is only local in special geometries and theories; generically it contains couplings between widely separated degrees of freedom inside $A$. This provides a stress test for any interpretation that assigns a local temperature or a local Gibbs form to a generic subregion: the correct modular generator can be highly nonlocal even in a simple free model, demonstrating that entanglement-induced thermality is not the same as ordinary local thermal physics.

The JSON results quantify nonlocality by aggregating the absolute weight of $h_{ij}$ by distance $d=|i-j|$ through $w(d)=\sum_{|i-j|=d}|h_{ij}|$, and reporting both a tail fraction beyond a “local” cutoff and an average coupling range. In this example, `tail_fraction_d_gt_2` is about 0.26 and `mean_distance` is about 2.20, meaning a substantial fraction of the modular coupling weight sits at distances larger than nearest and next-nearest neighbors, even though the largest single weights can still be near the diagonal. You should not over-interpret any particular entry like `h_0,49` as a signal of long-range dynamics; the modular kernel encodes entanglement structure inside $\rho_A$, not causal propagation. Instead, interpret the nonlocality measures as a coarse statement about how the logarithm in $h=\log((I-C_A)/C_A)$ reshuffles correlation-matrix eigenmodes into spatial couplings that need not be confined to short distances.

---

## Toy 078 — Edge modes as a factorization fix for gauge constraints

This toy builds a tiny “two-region” quantum system meant to mimic a gauge-theory cut: region A and region B each have a bulk qubit, and a boundary (edge) charge is constrained to match the bulk configuration. Concretely, the physical subspace is enforced by a proxy Gauss-law relation $Z_{\mathrm{edge}} = Z_{A} Z_{B}$, where $Z$ is the Pauli-$Z$ operator on the indicated qubit, so the edge qubit records the $\pm 1$ product of the two bulk $Z$-eigenvalues. The quantity of interest is the reduced state associated to region A (and its entropy), comparing what you get if you “just trace out B” versus what you get if you explicitly include and then trace over the boundary data.

The toy exists to stress-test the notion that “subsystem entanglement = trace over the complement” in the presence of a constraint tying degrees of freedom across the cut. In gauge systems, the constraint is the point: it couples A and B so that the physical Hilbert space is not a simple tensor product, and blindly applying a partial trace can be ambiguous or conceptually inconsistent even if it produces a matrix. The edge-mode extension is a bookkeeping device that exposes this: it enlarges the Hilbert space so that a tensor-product factorization is restored by construction, while the constraint information is carried by the boundary qubit, i.e. the “missing” data implied by $Z_{\mathrm{edge}} = Z_{A} Z_{B}$ is made explicit rather than silently assumed.

To interpret the JSON, focus on the reported entropies and eigenvalues of the reduced density matrices, and on the trace distance between reduced states produced by the two constructions. In this particular run, the naive A-bulk state has eigenvalues near $(0.5, 0.5)$ and entropy $S_A \approx 0.693$ (i.e. $\ln 2$), and the “extended keep A+edge” spectrum shows two zero modes and two $0.5$ modes, indicating that the support lies in a constrained subspace even after extension. The key diagnostic is whether the extended procedure, after tracing out the edge to return to A-bulk, changes the local state: here the trace distance is $0.0$ and $S_{A,\mathrm{bulk}}$ matches the naive value, so this output should not be over-interpreted as “edge modes always change entanglement,” but rather as demonstrating how to check consistency—when the proxy constraint and chosen state make the naive and extended reductions coincide, both are simply compatible with the same constraint encoded by $Z_{\mathrm{edge}} = Z_{A} Z_{B}$.

---

## Toy 079 — Spectral Positivity Violation in Euclidean Correlators

This toy constructs and analyzes a Euclidean two-point correlator sampled at discrete Euclidean times and related to an underlying spectral density through the Laplace-type transform $G_E(t)=\int_0^\infty d\omega\,\rho(\omega)e^{-\omega t}$. The physical setup is deliberately simple: a smooth, strictly nonnegative “true” spectral density is used to generate a correlator, which is then mildly distorted by an oscillatory factor in Euclidean time. The quantity of interest is not the correlator itself, but the spectral density reconstructed from it, which in a unitary quantum field theory must remain nonnegative.

The toy exists to illustrate a sharp limitation of common approximation and inversion procedures rather than to model real dynamics. In a unitary, causal quantum field theory, the Källén–Lehmann representation guarantees $\rho(\omega)\ge 0$, so any violation of this condition signals that the object being analyzed cannot correspond to a physical Wightman function. By adding a small, smooth distortion that leaves the Euclidean correlator visually well behaved and easy to fit, the toy exposes how unconstrained inversions and regularization schemes can yield negative spectral weight even when residuals are small. This is a stress test for unitarity and positivity, not a prediction about spectra.

The JSON output should be read with this diagnostic purpose in mind. The downsampled arrays of $\rho_{\text{true}}$ and reconstructed $\rho_{\text{hat}}$ show where and how negativity appears, while summary scalars such as the minimum reconstructed value and the negative spectral weight fraction quantify its severity. A key trend to notice is that substantial negative weight can coexist with a good Euclidean fit, as indicated by the small residuals between reconstructed and distorted correlators. These numbers should not be over-interpreted as physical masses or probabilities; they only demonstrate how the inversion of $G_E(t)$ back to $\rho(\omega)$ can fail to respect the positivity implied by the defining relation when the input correlator is slightly non-physical.

---

## Toy 080 — Asymptotic OPE and the optimal-truncation boundary

This toy compares an “OPE-like” truncated local series to a well-defined target function, using the classic factorially divergent expansion $F(x)\sim\sum_{n=0}^\infty (-1)^n n!\,x^n$ as a stand-in for a locality expansion in a small parameter $x$ (think $x$ as a dimensionless separation ratio such as $(|x_{12}|/L)^\Delta$). Because the series diverges for any nonzero $x$, the toy defines an “exact” reference value via Borel summation, $F_{\mathrm{exact}}(x)=\int_0^\infty dt\,e^{-t}/(1+x t)$ for $x>0$, and then evaluates partial sums $S_N(x)$ and their absolute error as the truncation order $N$ is varied. The quantity of interest is not a prediction of a physical correlator, but the error curve $|S_N(x)-F_{\mathrm{exact}}(x)|$ and where it is minimized.

The toy exists to demonstrate a failure mode that often gets glossed over in intuitive uses of the OPE: increasing the number of retained terms is not guaranteed to monotonically improve accuracy, because the expansion is only asymptotic rather than convergent. The conceptual issue it exposes is a noncommuting-limits boundary between “take $x\to 0$ first” (where a few terms can be informative) and “take $N\to\infty$ first” (which is meaningless here), with the practical consequence that there is typically an optimal truncation order beyond which the series runs away. In this construction the optimal order scales roughly like $N_\star\sim 1/x$, so the toy frames OPE truncation as a stress test of local control: it illustrates that locality-style series can be excellent at low order and catastrophically wrong at higher order without any contradiction.

To interpret the JSON, use `observables.summary` to see, for each sampled $x$, the Borel-summed reference `F_exact`, the minimizing truncation `N_opt`, and the contrast between `min_error` and `error_at_Nmax` that quantifies the nonmonotonicity. The important trend is that as $x$ increases the optimal truncation drops (e.g. `N_opt` goes from 14 at $x=0.02$ to 2 at $x=0.3$) and the best achievable error worsens, while pushing to the fixed maximum order can explode the error by many orders of magnitude (dramatically for $x=0.1,0.2,0.3$). What should not be over-interpreted is the apparent “convergence” at very small $x$ (for $x=0.02$ the error at `Nmax=40` happens to remain tiny), because the toy’s message is about the shape of the error-vs-$N$ curve and the existence of an $N_{\mathrm{opt}}$ tied to the asymptotic series; the sample-point entries connect this directly by reporting `S_N`, `F_exact_borel`, and `abs_error` for each $(x,N)$ pair rather than asserting any global predictive power beyond the integral definition of $F_{\mathrm{exact}}(x)$.

---

