# Phase 7 — Truncation, Unitarity, & Lattice Effects

**Toys 061–070**

---

## Toy 061 — Truncation-induced unitarity breakdown in single-mode squeezing

This toy illustrates how an exactly unitary quantum evolution can appear non-unitary once a hard cutoff is imposed on the Hilbert space. The physical setup is a single bosonic mode prepared in a squeezed vacuum state, with a time-dependent squeeze parameter $r(t)$ that grows linearly in time. In the exact theory the state populates arbitrarily high occupation numbers, and a convenient orienting quantity is the exact mean occupation $\langle n\rangle=\sinh^2 r$, which grows rapidly as squeezing increases.

The toy exists to stress-test a common numerical practice rather than to make a physical prediction. By truncating the Fock space to occupations $n\le N_{\max}$, part of the wavefunction is forcibly discarded, so the retained state no longer has unit norm. This exposes a conceptual failure mode: even though the underlying dynamics is unitary, the truncated description shows norm loss and cutoff-dependent observables, a purely operational breakdown induced by truncation. The retained norm $Z(N_{\max})=\sum_{n\le N_{\max}}P(n)$ serves as a diagnostic of this leakage rather than a statement about fundamental non-unitarity.

The JSON results should be read as a map of how severe this truncation effect is across time and cutoff. The key indicators are the retained norm $Z$ (or equivalently the leakage $1-Z$) and the comparison between truncated and exact mean occupations, both before and after renormalizing by $Z$. Large leakage and large relative errors signal that significant probability weight lies beyond the cutoff, while convergence of these quantities as $N_{\max}$ increases indicates that the truncation is becoming harmless. What should not be over-interpreted is the absolute value of any truncated observable at small $N_{\max}$ or large $r$: these numbers do not represent physical predictions, but rather quantify how truncation distorts quantities that are well defined in the exact formulas.

---

## Toy 062 — Entanglement-driven loss of predictivity under local truncation

This toy demonstrates how entanglement growth alone can undermine the usefulness of a truncated description even when the full dynamics is exactly unitary. The setup consists of two identical bosonic modes coupled by a two-mode squeezing interaction, evolving from the joint vacuum so that entanglement builds up over time. A convenient orienting quantity is the squeezing parameter $r(t)=g t$, which controls the exact reduced-state mean occupation of a single mode, $\bar n=\sinh^2 r$, after tracing out its partner.

The purpose of the toy is to expose a limitation of reduced or approximate descriptions rather than to model a physical system faithfully. As the two modes become increasingly entangled, the reduced state of one mode spreads its probability over arbitrarily high occupation numbers, so any fixed local cutoff necessarily discards information. This illustrates a conceptual failure mode: although the global evolution remains unitary, local observables computed in a truncated Hilbert space become increasingly cutoff-dependent as entanglement grows, highlighting a stress test for predictivity rather than a breakdown of fundamental dynamics.

The JSON output should be interpreted as a quantitative record of how truncation quality degrades with time and cutoff. The retained probability $Z$ indicates how much of the reduced-state weight lies below the chosen $N_{\max}$, while the errors in the mean occupation and entropy show how sensitive these observables are to the cutoff. Large leakage and large relative errors signal that the truncation is no longer reliable, whereas convergence of these values with increasing $N_{\max}$ indicates recovery of the exact reduced-state formulas. What should not be over-interpreted is the apparent value of any truncated observable at late times and small cutoffs: these numbers do not predict physical outcomes, but instead measure how entanglement-driven spreading invalidates a finite local description relative to the exact expressions.

---

## Toy 063 — Reflection-positivity stress test via spectral negativity

This toy illustrates how violations of spectral positivity show up directly in Euclidean correlation functions. The physical setup is a proxy for the Källén–Lehmann representation of a two-point function, where a Euclidean correlator is written as a superposition of free massive contributions with different masses and weights. In this discrete approximation the Schwinger function is $C(t)=\sum_i w_i e^{-m_i t}$, with $t$ the Euclidean time, $m_i$ positive masses, and $w_i$ their spectral weights.

The toy exists to expose a sharp consistency requirement rather than to model realistic dynamics. In any unitary relativistic QFT with a positive-norm Hilbert space, the spectral density must be nonnegative, which implies reflection positivity and guarantees $C(t)\ge 0$ for all $t$. By allowing negative weights $w_i<0$, this construction deliberately probes a failure mode common to ad hoc effective propagators or higher-derivative modifications: even if such models look well behaved algebraically, they cannot correspond to a consistent underlying quantum theory.

The JSON results should be interpreted as diagnostics of this positivity test. The key quantities are the minimum value of $C(t)$ over the sampled grid, the fraction of Euclidean times where $C(t)$ becomes negative, and the flag indicating whether any negative spectral weights were present. Sustained positivity of $C(t)$ across the grid indicates that this particular discrete spectrum does not visibly violate reflection positivity on the sampled range, while any dip below zero would signal a breakdown tied directly to the weights in the sum. What should not be over-interpreted is the detailed shape of $C(t)$ or the precise location of any extrema: the numerical values only illustrate how the signs and magnitudes of the $w_i$ feed into the correlator through the exponential terms, not a prediction for a physical correlator.

---

## Toy 064 — Asymptotic perturbation series and optimal truncation

This toy illustrates how perturbative expansions in quantum field theory can be asymptotic rather than convergent. The setup is a deliberately simple formal series with coefficients growing factorially, so that for any nonzero coupling the infinite sum diverges even though early partial sums can give reasonable approximations. The quantity of interest is the formal series $S(g)=\sum_{n\ge 0} n!\,g^n$, which serves as a controlled stand-in for perturbative expansions affected by renormalons or instantons.

The toy exists to expose a conceptual limitation of perturbation theory, not to model a physical observable. In asymptotic series there is no sense in which “more loops” always improve accuracy: beyond an optimal order $n_\*$, successive terms grow and rapidly spoil the approximation. This is framed here as a stress test for predictivity, highlighting that different prescriptions—such as truncating at $n_\*$ or defining a Borel sum—differ by nonperturbative ambiguities of order $\exp(-\text{const}/g)$, rather than representing competing exact predictions.

The JSON results should be read as diagnostics of this behavior across different couplings. The optimal truncation order $n_\*$ is identified by the smallest term magnitude, and the corresponding partial sum is compared to a principal-value Borel resummation defined by $S_{\text{PV}}(g)=\text{PV}\int_0^\infty e^{-t}/(1-g t)\,dt$. Small errors at $n_\*$ indicate the best perturbative accuracy achievable, while dramatic blow-up of high-order partial sums shows the loss of predictivity beyond that point. The reported Borel ambiguity magnitude signals what should not be over-interpreted: discrepancies at or below this scale cannot be resolved within perturbation theory alone and should not be mistaken for meaningful physical differences.

---

## Toy 065 — Lattice dispersion as a microcausality leakage proxy

This toy illustrates how a UV regulator in the form of a spatial lattice deforms relativistic causal structure. The setup is a free scalar field on a 1D periodic lattice with spacing $a$, where normal modes have dispersive frequencies $\omega_k=\sqrt{m^2+(2/a)^2\sin^2(\pi k/N)}$ and unequal-time commutators no longer respect a sharp continuum light cone. The quantity of interest is the lattice commutator kernel $\Delta(n,t)$, evaluated as a discrete mode sum and compared against a continuum proxy cone defined by $|x|>t$ with $x=na$ and $c=1$.

The toy exists to stress-test the assumption that microcausality can be treated as exact in regulated or discretized calculations. In continuum relativistic QFT, vanishing commutators at spacelike separation are a structural consistency condition, but on a lattice the modified dispersion relation effectively replaces the sharp light cone with a broadened, regulator-dependent “causal cone.” This demonstrates a limitation: even when equal-time canonical commutation relations are enforced, the regulator induces small but nonzero commutator support outside the continuum cone, so any claim of exact microcausality must be treated as contingent on taking the continuum limit.

The JSON results should be interpreted as leakage diagnostics rather than absolute causal statements. For each sampled time $t$, the key fields are the leakage fraction outside the cone, defined as the ratio of $\sum_{|x|>t}|\Delta|$ to $\sum_{\text{all}}|\Delta| over the scanned window, and the maximum $|\Delta|$ found outside the cone, which flags the strongest individual violation in that region. Increasing leakage with time (as seen here up to a few percent, with a reported maximum leakage fraction around $3\times 10^{-2}$) indicates growing dispersive spillover relative to the total commutator weight in the window, while very small values at early times indicate near-continuum behavior on those scales. What should not be over-interpreted is the precise numerical value of $\Delta$ at any single site or time, since it depends on the chosen window $n_{\max}$, lattice spacing, and the continuum-cone proxy; the meaningful reading is the trend that the regulator’s $\omega_k$ controls where the mode sum places weight relative to the $|x|>t$ criterion.

---

## Toy 066 — Gauge-Fixed Propagator and Ward Identity Violation

This toy evaluates how a gauge-fixed propagator behaves under a Ward identity test in a simple Abelian gauge theory proxy formulated in Euclidean momentum space. The setup considers a free U(1) gauge field in a covariant gauge with parameter $\xi$, and the quantity of interest is the Ward residual measuring how closely the propagator respects current conservation. In the exact case, the propagator satisfies the identity $p_\mu D_{\mu\nu}(p)=\xi p_\nu/p^2$, so contracting the propagator with the momentum cleanly isolates the expected longitudinal contribution.

The toy exists to illustrate how small, controlled violations of gauge symmetry can manifest as measurable failures of Ward identities when a regulator or truncation distorts unphysical sectors. A deliberately “bad” modification is added only to the longitudinal part of the propagator, $D'_{\mu\nu}=D_{\mu\nu}+\varepsilon p_\mu p_\nu/(p^2+\Lambda^2)^2$, which spoils the exact cancellation required by gauge invariance. This construction is not a physical prediction but a stress test that exposes a common failure mode: even when transverse physics is untouched, longitudinal contamination can leak into diagnostics if gauge symmetry is imperfectly preserved.

The JSON output reports, for each sampled momentum, the Ward residual vector $W_\nu=p_\mu D'_{\mu\nu}-\xi p_\nu/p^2$, its norm, and a relative violation defined by comparison to the target longitudinal term. What matters is the systematic trend: the relative violation grows with momentum magnitude for fixed regulator parameters, reaching a maximum summarized in the output, while remaining largely insensitive to the direction of $p$. The sign structure of the vector components simply reflects the chosen momentum direction and should not be over-interpreted as physical anisotropy. These numerical values should be read only as indicators of how the added longitudinal distortion feeds into the Ward identity equation, not as evidence for real gauge-field dynamics.

---

## Toy 067 — Path-Integral Contour Choice and Stokes Jumps

This toy computes a zero-dimensional path integral for a single real variable with a quartic action, treating it as a minimal proxy for a quantum field theory path integral. The object of interest is the partition function $Z(g)=\int_C d\phi\,\exp(-(1/2)\phi^2-(g/4)\phi^4)$, where the integral is taken along a chosen contour $C$ in the complex $\phi$ plane. For positive coupling $g$, the real-axis contour converges and provides a natural reference value, while for other parameter choices the same formal expression requires deformation into the complex plane to remain well defined.

The purpose of the toy is to expose the fact that the path integral is not uniquely defined by the action alone: additional contour data are required. When $g<0$, the real-line integral diverges, but multiple complex contours satisfy the convergence condition $\mathrm{Re}(g e^{4 i\theta})>0$, and each admissible choice yields a different numerical result. As the parameters or contour angle $\theta$ are varied, the dominant saddle contributions can switch abruptly, illustrating a Stokes phenomenon. This is framed explicitly as a stress test of ambiguity rather than a statement about physical vacua or dynamics.

The JSON output lists, for each sampled coupling $g$ and contour angle $\theta$, whether the contour converges and the resulting complex value $Z_\theta$, reported via its real part, imaginary part, magnitude, and phase. For $g>0$, results are compared to the real-axis reference to show how strongly alternative contours deviate, while the spread of $|Z_\theta|$ across angles summarizes contour sensitivity. Extremely large magnitudes or spreads indicate strong Stokes-like behavior and numerical dominance of different saddles, not meaningful physical divergences. These values should be interpreted only as diagnostics of how the contour-dependent integral realizes the defining equation for $Z(g)$, and not as predictions of observable quantities.

---

## Toy 068 — IR Dressing and Infraparticle Orthogonality Catastrophe

This toy models how a charged “bare” Fock state fails to overlap with a physically dressed charged state in the presence of massless gauge quanta, using a controlled infrared (IR) cutoff $\Lambda_{\mathrm{IR}}$. The quantity of interest is the overlap probability between a bare and an IR-dressed state, taken to scale as $P_{\mathrm{overlap}}=(\Lambda_{\mathrm{IR}}/\Lambda_{\mathrm{ref}})^A$, where $A>0$ is a tunable proxy for coupling/kinematics and $\Lambda_{\mathrm{ref}}$ sets a reference scale. As the cutoff is lowered, the model forces the overlap toward zero, reflecting the idea that the dressed state carries an ever-growing cloud of soft radiation.

The toy exists to demonstrate a limitation of naive asymptotic-state and S-matrix reasoning in gauge theories with massless bosons: exclusive “one-particle” charged states are not stable, well-defined objects as the IR regulator is removed. The stress test is the orthogonality catastrophe: even if the underlying dynamics are otherwise benign, the overlap can vanish purely because infinitely many arbitrarily soft quanta contribute. The same mechanism implies a logarithmically divergent soft content, captured here by $N_{\mathrm{soft}}=A\ln(\Lambda_{\mathrm{ref}}/\Lambda_{\mathrm{IR}})$, which increases without bound as $\Lambda_{\mathrm{IR}}\to 0$ and signals that the Fock description is the wrong basis for exact charged asymptotic states.

The JSON results should be read as a parametric scan over $\Lambda_{\mathrm{IR}}$ showing two linked trends: $P_{\mathrm{overlap}}$ decreases as a power law in $\Lambda_{\mathrm{IR}}$ while $N_{\mathrm{soft}}$ grows linearly in $\ln(1/\Lambda_{\mathrm{IR}})$. The most informative magnitudes are the smallest-cutoff overlap (reported in the summary) and the corresponding largest $N_{\mathrm{soft}}$, plus the log-slope $\mathrm{d}\ln P/\mathrm{d}\ln\Lambda_{\mathrm{IR}}=A$ which should remain constant if the proxy scaling holds. Individual numeric values (e.g., the absolute size of $P_{\mathrm{overlap}}$ at a particular cutoff) should not be over-interpreted as physical probabilities for a specific detector or process; they only illustrate how the overlap equation and the soft-number equation track one another conceptually, with the same coefficient $A$ governing both decay and soft growth.

---

## Toy 069 — Stress-Tensor Noise and Mean-Field Predictivity Loss

This toy models a single metric perturbation mode $h(t)$ as a damped oscillator driven by a mean stress tensor plus fluctuations, as a proxy for semiclassical gravity where geometry responds to quantum matter. The physical setup is a linear response equation $ \ddot h + 2\gamma \dot h + \omega_0^2 h = \kappa[\langle T\rangle + \xi(t)] $, with constant $\langle T\rangle=T_{\mathrm{mean}}$ and a zero-mean stochastic source $\xi(t)$ representing stress-tensor noise. The quantity of interest is not a “predicted metric,” but how the variance of $h$ induced by $\xi$ competes with the mean response driven by $\langle T\rangle$.

The toy exists to stress-test the limitation of mean-field semiclassical gravity: $G_{ab}=8\pi\langle T_{ab}\rangle$ can look well-posed while ignoring fluctuations that can dominate the response of metric perturbations. Here the conceptual failure mode is that even if the mean solution is smooth, the noise-driven second moment can grow to the point that the mean trajectory is no longer representative, mirroring the motivation for Einstein–Langevin (stochastic gravity) treatments. The toy frames this as predictivity loss rather than an instability claim about real spacetimes, using the criterion that the signal-to-noise ratio $\mathrm{SNR}=|h_{\mathrm{mean}}|/\sqrt{\mathrm{Var}[h]}$ dropping below one indicates that fluctuations are as large as (or larger than) the mean-field signal.

To interpret the JSON, follow each time sample’s pair $(h_{\mathrm{mean}}(t),\mathrm{Var}[h](t))$ and the derived SNR: increasing $\mathrm{Var}[h]$ and decreasing SNR are the key signatures that the noise channel is eroding mean-field usefulness. The summary fields tell you the maximum variance encountered and the first time where $\mathrm{SNR}<1$ (here reported as $t=6.0$), which is the toy’s operational marker of “predictivity loss” for this parameter choice. Do not over-interpret oscillations of $h_{\mathrm{mean}}$ or later SNR recovery as physical stabilization; they reflect the damped-oscillator Green’s function used in the variance integral and the fixed OU correlation model for $\xi(t)$. Conceptually, the reported $\mathrm{Var}[h]$ is the deterministic double-correlation-weighted response of the same equation that defines $h_{\mathrm{mean}}$, so the JSON values should be read as illustrating how the stochastic-source term in the equation competes with the mean forcing, not as a forecast of quantum-gravity dynamics.

---

## Toy 070 — IR Superselection and Sector Explosion

This toy models a family of infrared (IR) superselection sectors labeled by a continuous parameter $\alpha\in[0,1]$, representing soft charges or dressing data in theories with long-range fields. The physical setup is encoded in the overlap between states from different sectors, taken to decay as $|\langle \alpha|\beta\rangle|^2=\exp(-C|\alpha-\beta|^2/\Lambda_{\mathrm{IR}})$, where $C>0$ sets the sharpness of separation and $\Lambda_{\mathrm{IR}}$ is an IR regulator. As the regulator is lowered, sectors that were weakly distinguishable become effectively orthogonal, mimicking how different soft dressings define distinct asymptotic states.

The toy exists to illustrate a conceptual pressure point rather than to predict dynamics: the possibility that the physical state space is not a single separable Hilbert space with one vacuum, but a direct integral over continuously labeled superselection sectors. In this stress test, no finite-energy or local operator can connect different $\alpha$ sectors once $\Lambda_{\mathrm{IR}}\to 0$, even though local measurements in a bounded region are insensitive to $\alpha$. The resulting “sector explosion” exposes an ambiguity in how one counts degrees of freedom and defines completeness of states when IR structure and asymptotic symmetries are taken seriously.

The JSON results should be read as diagnostics of this sector structure under discretization. For each choice of $\Lambda_{\mathrm{IR}}$ and number of bins $M$, the Gram matrix eigenvalues summarize how many sectors are effectively orthogonal: the effective rank and participation ratio increase as $\Lambda_{\mathrm{IR}}$ decreases, while average off-diagonal overlap drops toward zero. The key trend is that at small $\Lambda_{\mathrm{IR}}$ the effective rank approaches $M$, indicating many mutually orthogonal sectors, but this should not be over-interpreted as a literal dimension of a physical Hilbert space. Instead, these numbers connect back to the overlap formula by showing how the exponential suppression in $|\langle \alpha|\beta\rangle|^2$ translates into an operational proliferation of superselection sectors in the regulated model.

---

