# Phase 1 — Free Field Theory & UV/IR Regulation

**Toys 001–010**

---

## Toy 001 — Free Scalar QFT in 1+1D: Regulated Vacuum Two-Point Function and Microcausality

This toy computes two standard vacuum correlators for a free real scalar field $\phi(t,x)$ in flat 1+1D Minkowski spacetime, using a hard momentum cutoff $\Lambda$ and a discrete step $dk$ to approximate the mode integrals. The primary quantity of interest is the regulated vacuum Wightman function $G(t,x)$, reported as a real-valued cosine integral that captures the short-distance and oscillatory structure of vacuum correlations. Concretely, it implements the cutoff-sensitive mode sum $G(t,x)=\int_{-\Lambda}^{\Lambda}\frac{dk}{4\pi\,\omega_k}\cos(\omega_k t-kx)$, where $\omega_k=\sqrt{k^2+m^2}$ is the relativistic dispersion relation for mass $m$.

The toy exists to expose a stress point that appears even before any interactions: "local" field correlators are not well-defined without a UV regulator, and naive numerical evaluations can make causality tests look misleading unless you understand what the regulator is doing. In particular, the coincident-point correlator $G(0,0)$ is not a physical prediction here—it is a regulated diagnostic whose magnitude reflects UV sensitivity and will generally grow (or otherwise change nontrivially) as $\Lambda$ is increased or $dk$ is refined. Alongside this, the toy computes the Pauli–Jordan commutator function as a microcausality check, $\Delta(t,x)=\int_{-\Lambda}^{\Lambda}\frac{dk}{2\pi\,\omega_k}\sin(\omega_k t)\sin(kx)$, which should vanish for spacelike separation in the continuum theory; at finite cutoff and finite step size it is only expected to be numerically small, not identically zero.

To interpret the JSON, focus on each entry in `sample_points`: `interval_t2_minus_x2` tells you whether the separation is timelike or spacelike, and `wightman_function_G` and `commutator_Delta` are the corresponding regulated values at that separation. For microcausality, the sign of `commutator_Delta` is not the point—what matters is whether its magnitude stays near numerical noise for spacelike points (`spacelike: true`) and how that magnitude changes as you vary the regulator parameters (`cutoff_Lambda`, `dk`); values on the order of $10^{-15}$ here indicate cancellation at the level of floating-point precision rather than an exact statement at finite $\Lambda$. For UV sensitivity, compare `wightman_function_G` across separations and especially note that the coincident-point entry marks `coincident_limit_defined: false`: the reported number is a regulated proxy for the divergent behavior of $G(t,x)$ as $(t,x)\to(0,0)$ and should not be over-interpreted as a physical observable; instead, treat it as evidence that the integral definitions of $G$ and $\Delta$ are being sampled with a cutoff and that their reported magnitudes must be read in the light of those same integrals.

---

## Toy 002 — Unruh–DeWitt Detector: Observer-Dependent "Thermality" in Flat Spacetime

This toy computes the response of an idealized Unruh–DeWitt point detector coupled to a free massless scalar field in 1+1D Minkowski spacetime, comparing what an observer "sees" along an inertial versus a uniformly accelerated worldline. The quantity of interest is a regulated, finite-time response functional that integrates the field's vacuum two-point function along the detector's proper time and weights it by the detector's energy gap $\Omega$. In code terms it evaluates a double integral of the form $R(\Omega)\propto\int d\tau\int d\tau'\cos(\Omega(\tau-\tau'))\,G(\tau,\tau')$, where $G(\tau,\tau')$ is the Minkowski-vacuum Wightman function pulled back to the chosen trajectory.

The toy exists to demonstrate a conceptual failure mode of "particle counting" as an observer-independent notion: the same global QFT vacuum can look qualitatively different to different families of observers. Uniform acceleration introduces an effective horizon and, for this detector model, produces a response consistent with thermality even though the spacetime is perfectly flat—so the effect is not a curvature prediction but a stress test of how "particles" enter through detector/observer definitions. The interpretive anchor is the Unruh temperature $T=a/(2\pi)$ (with acceleration $a$), which is used here only as a diagnostic benchmark for what an accelerated detector is expected to resemble in the long-time, well-regulated limit.

To read the JSON, treat `local_observables.response_function` as the computed finite-time, cutoff-regulated number proportional to the detector's excitation probability for the specified parameters (`gap_Omega`, `acceleration_a`, `tau_max`, `dtau`, `cutoff_Lambda`). The fields `causal_structure.has_horizon` and `causal_structure.unruh_temperature` tell you whether the trajectory is accelerated and what benchmark temperature $T=a/(2\pi)$ corresponds to that acceleration; in this sample the nonzero temperature (about 0.159) and a positive response value are the qualitative signals to look for. What you should not over-interpret is the absolute magnitude (e.g., 5.16 here) as a physical rate or spectrum: it depends on the finite switching window, discretization, and UV cutoff, and the toy does not extract a full thermal spectrum—rather it connects the observed response back to the integral definition of $R(\Omega)$ and the diagnostic expectation that acceleration tends to enhance excitations in a way consistent with the temperature scale set by $T=a/(2\pi)$.

---

## Toy 003 — Finite-Volume Free Scalar: Boundary and IR Sensitivity of Vacuum Correlations

This toy computes an equal-time vacuum two-point correlator for a free real scalar field $\phi$ in 1+1D, but with space compactified into a periodic box of length $L$ (topology $S^1$). The quantity of interest is the regulated finite-volume correlator at a fixed spatial separation $x$ (here $x=1$), evaluated by summing discrete momentum modes allowed by the periodic boundary condition. In this setup the correlator is implemented as $G(x;L)=\sum_{n=-n_{\max}}^{n_{\max}}\frac{1}{2L\,\omega_{k_n}}\cos(k_n x)$ with $k_n=2\pi n/L$ and $\omega_{k_n}=\sqrt{k_n^2+m^2}$ for mass $m$.

The toy exists to illustrate that even in flat spacetime, "the vacuum" is not a strictly local notion: changing global boundary conditions or infrared structure changes which modes define the vacuum and therefore changes correlators. The stress test is not about predicting a new force or effect, but about exposing the ambiguity that arises when one tries to treat a vacuum correlator as purely local data independent of the box size, topology, or mode spectrum. Put differently, by holding the local probe scale $x$ fixed while varying the global scale $L$ (and thus the infrared spacing of modes via $k_n=2\pi n/L$), the toy demonstrates how vacuum observables inherit sensitivity to IR and boundary choices.

To interpret the JSON, read each entry in `sample_points` as "same local probe, different global IR": `coordinates.x` is fixed at 1.0 while `coordinates.box_length_L` changes, and the reported value `local_observables.equal_time_correlator_G` is the corresponding mode-sum result. The important trend is whether $G(x;L)$ stabilizes as $L$ increases (here it drifts slightly from about 0.06740 at $L=10$ to about 0.06725 at $L=40$), which signals how strongly the correlator is still feeling the finite-volume infrared structure at that separation. What should not be over-interpreted is the last-digit difference between nearby $L$ values as a physical "effect size" on its own, because the result also depends on the mode cutoff `mode_cutoff_n_max` and the correlator definition itself (the discrete sum that approximates the continuum integral); the JSON numbers are best read as diagnostics of how the finite-volume expression for $G(x;L)$ changes when the mode spectrum $k_n=2\pi n/L$ is altered.

---

## Toy 004 — Vacuum Entanglement Entropy: UV-Dominated "Area Law" on a Lattice

This toy computes the ground-state entanglement entropy of a free real scalar field in 1+1D using a lattice discretization, where the lattice spacing $a$ acts as an explicit UV regulator. The physical setup is a chain of $N$ coupled harmonic oscillators representing the discretized field, bipartitioned into a contiguous region of a fixed number of sites (an "interval" on the lattice) and its complement. The quantity of interest is the von Neumann entropy of the reduced state of that region, extracted from the ground-state covariance matrices and conceptually tied to the symplectic spectrum of the restricted correlators via $M=C_x C_p$, where $C_x$ and $C_p$ are the field and momentum covariances.

The toy exists to illustrate that entanglement entropy in QFT is not a finite, well-defined observable without a UV cutoff: in the continuum limit the entropy diverges, reflecting short-distance correlations across the partition boundary. The setup exposes this UV sensitivity explicitly by varying the lattice spacing and demonstrating how the entropy grows logarithmically (in 1+1D) as the regulator is refined. This is not a failure of entanglement as a concept but a stress test showing that it cannot be cleanly localized without reference to some short-distance scale.

To read the JSON, focus on each entry's `local_observables.entanglement_entropy_S_vN`: as `lattice_spacing_a` decreases (the UV cutoff increases), the entropy should grow, with the functional form expected to follow roughly $S\sim c\log(L/a)$ for a region of size $L$. The important signal is the trend with $a$, not the absolute magnitude at any given cutoff, because the entropy depends on the precise lattice model (nearest-neighbor coupling, harmonic approximation, etc.) and the chosen region size. What should not be over-interpreted is the entropy as a physical "amount of information" at a single cutoff—it is a regulated diagnostic illustrating that entanglement has no intrinsic scale and that any finite entropy value is tied to the chosen regulator.

---

## Toy 005 — Vacuum Energy Density: Regulator Dependence of Zero-Point Energy

This toy computes the vacuum energy density for a free real scalar field in flat spacetime using a hard momentum cutoff $\Lambda$ as a UV regulator. The physical setup is the familiar sum-over-modes expression for zero-point energy: each mode contributes $\tfrac{1}{2}\omega_k$, and the total energy density is schematically $\rho_{\mathrm{vac}}\sim\int d^d k\,\omega_k/(2(2\pi)^d)$ up to the cutoff. In 1+1 dimensions with dispersion $\omega_k=\sqrt{k^2+m^2}$, the energy density is evaluated numerically by integrating $\omega_k$ over $|k|<\Lambda$.

The toy exists to expose the conceptual ambiguity in treating vacuum energy as a physical observable: the reported value grows rapidly with $\Lambda$, making it impossible to assign a unique "ground state energy" without specifying a regulator and a renormalization prescription. This is not a failure of the calculation but a stress test of what "vacuum energy" means in QFT, illustrating that the numerical magnitude depends entirely on the chosen cutoff and that the energy density has no intrinsic, regulator-independent value in the continuum theory.

To interpret the JSON, compare the `local_observables.vacuum_energy_density` across different cutoff values `parameters.cutoff_Lambda`. The energy density should increase roughly as $\Lambda^2$ in 1+1D (higher powers in higher dimensions), reflecting the UV-dominated nature of the integral. The important feature is the strong regulator dependence, not the absolute magnitude at any given $\Lambda$, because the vacuum energy has no physical meaning until counterterms or other subtraction schemes are specified. These numbers should not be over-interpreted as physical energy scales—they are diagnostic of how the mode sum diverges, conceptually connecting the reported JSON values back to the expression $\rho_{\mathrm{vac}}\propto\int_0^\Lambda dk\,\omega_k$ and its sensitivity to the cutoff.

---

## Toy 006 — φ^4 One-Loop Self-Energy: UV Sensitivity Despite "Small" Coupling

This toy computes the one-loop self-energy correction for a scalar field in $\phi^4$ theory using a momentum cutoff $\Lambda$ as a regulator. The setup considers the simplest scalar self-energy diagram at one-loop order, schematically of the form $\Pi(p^2)\sim\lambda\int d^d k/(k^2-m^2+i\epsilon)$ in $d$ spacetime dimensions, evaluated here in 1+1D for a tadpole-type contribution. The quantity of interest is the momentum-independent part of the self-energy, which enters as a mass shift $\delta m^2\sim\lambda\int dk/\omega_k$ up to the cutoff.

The toy exists to stress-test the notion that "small coupling" automatically guarantees perturbative control: even though $\lambda$ may be numerically small, the loop integral can be large or divergent due to UV contributions, making the one-loop correction comparable to or larger than the tree-level mass. This exposes a conceptual limitation of naive perturbation theory without renormalization, illustrating that the loop expansion is not reliably organized by powers of $\lambda$ alone when regulator-dependent integrals are present.

To read the JSON, examine how the reported self-energy correction `local_observables.self_energy_correction_delta_m2` changes with the cutoff `parameters.cutoff_Lambda` and the coupling `parameters.coupling_lambda`. The correction should grow with $\Lambda$ (logarithmically in 1+1D for this setup), and even for small $\lambda$ the magnitude can exceed naive expectations if the cutoff is large. The key signal is the regulator dependence and the relative size compared to the input mass, not the absolute numerical value, which depends on the chosen diagram and field definitions. These numbers should not be over-interpreted as physical mass shifts—they are diagnostic of UV sensitivity in perturbative QFT, conceptually illustrating how $\delta m^2\propto\lambda\log(\Lambda/m)$ links the reported values back to the loop integral structure.

---

## Toy 007 — Spectral Positivity Stress Test

This toy constructs and analyzes a Euclidean two-point correlator sampled at discrete Euclidean times and related to an underlying spectral density through the Laplace-type transform $G_E(t)=\int_0^\infty d\omega\,\rho(\omega)e^{-\omega t}$. The physical setup is deliberately simple: a smooth, strictly nonnegative "true" spectral density is used to generate a correlator, which is then mildly distorted by an oscillatory factor in Euclidean time. The quantity of interest is not the correlator itself, but the spectral density reconstructed from it, which in a unitary quantum field theory must remain nonnegative.

The toy exists to illustrate a fundamental consistency requirement: unitarity in Lorentzian signature demands that the spectral density satisfy $\rho(\omega)\ge 0$ for all $\omega>0$. By deliberately introducing a small violation of this positivity through a deformed correlator, the toy exposes how spectral reconstruction can diagnose whether a proposed two-point function is compatible with a unitary theory. This is not a claim about real QFT breaking down, but a stress test for numerical or phenomenological models that might generate correlators violating unitarity without making this explicit.

The JSON output should be read as a sampled scan over energy, where each entry reports both the undeformed and deformed spectral densities and whether they satisfy positivity at that point. The key signals are the sign of `rho_deformed` and the corresponding `positivity_deformed` flag once the threshold $\omega \ge m$ is crossed; oscillations in magnitude are expected, but any negative values directly indicate a breakdown of spectral positivity. These numbers should not be over-interpreted as physical spectral weights or probabilities, nor should their detailed shape be treated as predictive. Instead, they should be connected back conceptually to the inequality $\rho(\omega)\ge 0$, illustrating how the sampled JSON values act as a concrete diagnostic of whether a proposed two-point function can admit a unitary spectral representation.

---

## Toy 008 — Renormalization-Group Flow and Scheme Dependence

This toy computes a simple renormalization-group (RG) flow for a scalar $\phi^4$ coupling as a function of the renormalization scale $\mu$, using a deliberately simplified beta function. Starting from an initial coupling $\lambda_0$ at a reference scale $\mu_0$, the coupling is evolved according to $d\lambda/d\ln\mu = b\,\lambda^2$, where $b$ is a fixed coefficient controlling the rate of running. Two renormalization schemes are then compared: scheme A follows this flow directly, while scheme B applies an additional finite shift to the coupling at each scale, mimicking the freedom inherent in choosing different subtraction conventions.

The purpose of the toy is to expose the conceptual ambiguity in statements like "the value of the coupling constant." Even when the same beta function governs the logarithmic running, finite scheme-dependent terms lead to different numerical values of $\lambda(\mu)$ at the same scale. This is not a physical inconsistency but a stress test of interpretation: the toy demonstrates that couplings themselves are not observables, and that agreement at one scale does not imply agreement at another unless the full RG relation is specified. The setup illustrates a limitation of naive parameter comparisons rather than making any claim about real $\phi^4$ dynamics.

The JSON results should be read scale by scale, with each entry listing the renormalization scale $\mu$, the coupling in scheme A, the coupling in scheme B, and their difference. The important features are the relative offset between the two schemes and how this offset changes smoothly with $\mu$, while both flows remain finite and avoid a Landau pole over the sampled range. Absolute magnitudes of the couplings or the small numerical differences should not be over-interpreted as physical effects; they simply encode the scheme-dependent finite terms. Conceptually, the reported values are concrete samples of the RG solution $\lambda(\mu)=\lambda_0/[1-b\,\lambda_0\ln(\mu/\mu_0)]$, illustrating how different schemes map the same underlying flow into different numerical parametrizations.

---

## Toy 009 — Microcausality Breakdown under Regulator Deformation

This toy evaluates the Pauli–Jordan commutator for a free scalar field in 1+1 dimensions at a fixed spacelike separation, comparing a standard relativistic dispersion relation to a deformed, Lorentz-violating one. The physical setup fixes two spacetime points separated by time $t$ and space $x$ with $t^2-x^2<0$, where exact microcausality requires the field commutator to vanish. Numerically, the commutator is approximated as a momentum integral with a hard cutoff, schematically $\Delta(t,x)\sim\int dk\,\sin(\omega_k t)\sin(kx)/\omega_k$, where $\omega_k$ is the chosen dispersion relation.

The toy exists to stress-test the assumption that causality automatically survives regulator choices or small deformations. In the exact continuum theory with Lorentz invariance, the Pauli–Jordan function vanishes identically outside the light cone, but this property relies delicately on cancellations across all momenta. By introducing a cutoff and a modified dispersion relation, the toy exposes how these cancellations can fail, leading to nonzero spacelike commutators even in an otherwise free theory. This illustrates a limitation of regulated or deformed models rather than a prediction of superluminal signaling in nature.

The JSON output reports the commutator evaluated at one spacelike point for both the standard and deformed dispersions, along with the invariant interval $t^2-x^2$ confirming spacelike separation. The key quantity to inspect is the relative magnitude of `commutator_standard` versus `commutator_deformed`; their small but nonzero values reflect numerical and regulator effects, with differences indicating altered cancellations in the integral. These magnitudes should not be over-interpreted as physical signal strengths, since they depend on cutoff size and discretization. Conceptually, they connect back to the integral representation of $\Delta(t,x)$, illustrating how modifying $\omega_k$ disrupts the exact vanishing required by microcausality.

---

## Toy 010 — Inequivalent Vacua and Bogoliubov Transformations

This toy illustrates how different choices of mode decomposition for the same free scalar field lead to different vacuum states and particle interpretations. Two sets of creation and annihilation operators are related by a Bogoliubov transformation with coefficients $\alpha$ and $\beta$, and the quantity of interest is the particle number operator defined in one basis evaluated in the vacuum of the other. In this setup, the expectation value takes the simple form $\langle 0_A | N_B | 0_A \rangle = |\beta|^2$, showing that what one observer calls a vacuum can appear populated with particles to another.

The toy exists to expose the conceptual limitation of treating particle number as a fundamental or invariant concept in quantum field theory. Even though the underlying field algebra is unchanged, different mode choices correspond to inequivalent Fock space constructions, and there is no preferred notion of particles without additional structure. This serves as a stress test for intuition imported from finite-dimensional quantum mechanics, where all representations are unitarily equivalent, highlighting instead that field theories admit genuinely different vacua related by Bogoliubov transformations.

The JSON results list several mode indices, each reporting the same particle number expectation value determined by the chosen Bogoliubov coefficient $\beta$. The key feature is that this value is nonzero and identical across modes, signaling a systematic mismatch between the two vacua rather than a mode-specific effect. These numbers should not be over-interpreted as physical particle counts or energies; they simply encode how one basis re-expresses the vacuum of another. Conceptually, the reported values connect directly back to the relation between mode operators implied by $\alpha$ and $\beta$, illustrating how vacuum inequivalence manifests quantitatively in particle number expectations.

---
