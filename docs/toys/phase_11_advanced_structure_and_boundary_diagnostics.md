# Phase 11 — Advanced Structure & Boundary Diagnostics

**Toys 091–100**

---

## Toy 091 — Källén–Lehmann Spectral Reconstruction Is Ill-Posed Under Noisy Euclidean Data

This toy builds a Euclidean-time correlator from a positive “ground-truth” spectral density and then tries to reconstruct that spectrum back from the correlator. The physical setup is the (discretized) Källén–Lehmann/Laplace representation of a Euclidean two-point function, where the quantity of interest is the spectral density $\rho(\omega)$ that generates the correlator $G(t)$ via $$G(t)=\int_{0}^{\infty}\rho(\omega)e^{-\omega t}d\omega.$$ In the implementation, $\omega$ and $t$ are put on grids and the integral becomes a linear map $G\approx K\rho$ with $K_{ij}=e^{-\omega_j t_i}\Delta\omega$, then a small deterministic oscillatory multiplicative perturbation is applied to produce a “noisy” dataset.

The toy exists to demonstrate a conceptual limitation: inferring real-time/spectral content from finite, noisy Euclidean data is an ill-posed inverse problem, so “good fits” in correlator space do not imply a unique or trustworthy spectrum. It stresses how regularization choices act like priors: the code reconstructs $\rho$ by minimizing a Tikhonov objective with a smoothness penalty and a nonnegativity projection, $$\rho_{\hat{}}=\arg\min_{\rho\ge0}\|K\rho-G_{\mathrm{noisy}}\|^{2}+\alpha\|L\rho\|^{2},$$ where $L$ is a discrete second-derivative operator and $\alpha$ controls how strongly smoothness is enforced. The point is not to predict a physical spectrum, but to expose that small changes in $\alpha$ (or any comparable prior/constraint) can yield mutually incompatible $\rho(\omega)$ while leaving $K\rho$ almost unchanged within the noise level.

To interpret the JSON, treat it as a “many answers fit the same data” report rather than a measurement: `rel_fit_error_to_G_noisy` tells you how well each reconstructed spectrum reproduces the provided Euclidean correlator, while `top_peaks` and `rho_L2` show how the spectral shape and apparent peak structure move as $\alpha$ changes. In the provided run, the fit error stays small (best $\approx1.4\times10^{-3}$, worst $\approx9.0\times10^{-3}$) even as the spectra separate strongly (`max_pairwise_rho_L2_distance` $\approx6.62$), and the leading peak locations drift from a sharp, high-amplitude structure at small $\alpha$ toward a broader cluster at larger $\alpha$; that pattern is the intended signal of non-uniqueness. What should not be over-interpreted are the detailed peak positions, counts, or heights in `top_peaks` as “the true spectrum”: those are regularization-dependent artifacts that can change dramatically without spoiling $K\rho\approx G_{\mathrm{noisy}}$, because the Laplace kernel $e^{-\omega t}$ smears spectral features and makes inversion unstable, so the JSON values should be read as a consistency/instability diagnostic anchored to the mapping $G\approx K\rho$, not as ground truth about $\rho(\omega)$ itself.

---

## Toy 092 — Ward Identity Sensitivity to Regulator Choice

This toy evaluates a proxy for a gauge-theory Ward identity by comparing two loop-like integrals that should be related if gauge invariance is respected. The physical setup imitates the vacuum polarization structure, where transversality would imply $p_\mu \Pi_{\mu\nu}(p)=0$, but instead of computing a real tensor the code uses two scalar integrals $I_1(\Lambda,p)$ and $I_2(\Lambda,p)$ built from the same denominators and a simplified angular average. The quantity of interest is not either integral alone, but the residual of the expected linear relation between them, summarized by a calibrated target ratio $R$ and the Ward residual $W=|I_2-R I_1|$, evaluated as functions of the UV cutoff $\Lambda$ and external momentum $p$.

The toy exists to expose scheme dependence: gauge symmetry constraints are not automatically preserved by all regulators, and enforcing them can require counterterms whose form is not uniquely fixed by low-energy information. By contrasting a “hard cutoff” scheme with a “dimreg-like” proxy that subtracts leading power and logarithmic divergences, the setup illustrates how the same underlying integral can either respect or violate the Ward identity depending on the regularization prescription. This is framed explicitly as a stress test, not a calculation of physical amplitudes: the angular averaging deliberately breaks exact gauge structure while preserving ultraviolet behavior, making regulator-induced violations visible rather than hiding them.

The JSON results should be read as a comparative audit across schemes and scales. For each $(\Lambda,p)$ point, the key numbers are the `ward_residual` entries and the corresponding `counterterm_fix_delta`, which quantify how badly the identity is violated and how much would need to be added to restore it in that scheme. Trends matter more than absolute values: the hard cutoff consistently shows larger residuals (with a maximum around $1.76\times10^{3}$) than the dimreg proxy (maximum around $1.29\times10^{3}$), even though both grow with $\Lambda$ and vary with $p$. What should not be over-interpreted are the specific magnitudes of $I_1$, $I_2$, or the calibrated ratio $R$ as physical predictions; they are artifacts of the proxy construction, meant only to connect the numerical Ward residuals back to the conceptual relation $I_2\approx R I_1$ and to demonstrate how regulator choice controls whether that relation holds.

---

## Toy 093 — Inequivalent Vacua from Bogoliubov Overlap Collapse

This toy studies how the vacuum state of a system changes under a Bogoliubov (squeezing) transformation when the number of modes grows. The physical setup is a collection of harmonic-oscillator modes labeled by $k=1,\dots,N$, where an “interacting” vacuum is defined by mode-dependent squeeze parameters $r_k$ that depend on a control strength $g$ and a suppression scale $k_0$. The quantity of interest is the overlap between the reference vacuum and the squeezed vacuum, which factorizes over modes and has logarithm $$\log|\langle 0|0_g\rangle|=-\tfrac12\sum_{k=1}^{N}\log(\cosh r_k),$$ serving as a proxy for how close the two vacua are in Hilbert space.

The toy exists to illustrate a conceptual boundary behind Haag-type obstructions: in the continuum or infinite-volume limit, vacua related by Bogoliubov transformations can become orthogonal, indicating inequivalent Hilbert-space representations. Rather than claiming a physical interaction, the model is a stress test of representation continuity, using a deterministic choice $r_k=g/(1+(k/k_0)^2)$ that is strongest in the infrared and suppressed in the ultraviolet. This framing emphasizes that even mild per-mode squeezing can accumulate across many modes, so that increasing $N$ can qualitatively change the relationship between vacua without any sharp transition at finite $N$.

To interpret the JSON results, focus on how `log_overlap` and `overlap` vary with $N$ and $g$, and on whether the `inequivalent_representation_proxy` flag ever turns true when the overlap drops below the chosen threshold. In the provided data, overlaps decrease rapidly with increasing $g$ but stabilize as $N$ grows, and none fall below the threshold $10^{-6}$ within the sampled range, which is why all summary entries report `null` for the first $N$ crossing. The important trend is the saturation of the log-overlap at large $N$, reflecting the ultraviolet suppression in $r_k$; what should not be over-interpreted is the absence of orthogonality here as a statement about real quantum fields, since both the squeeze profile and the cutoff are toy choices meant only to connect the numerical overlaps back to the sum over $\log(\cosh r_k)$ and to demonstrate how inequivalence can emerge in principle.

---

## Toy 094 — RG Scheme Dependence Between Fixed Points and Cycles

This toy integrates a simple two-coupling renormalization-group (RG) flow to examine how qualitative flow features depend on the chosen scheme. The physical setup is an abstract coupling space with coordinates $(g,h)$ evolving in RG “time” $l$, governed by a cubic core flow supplemented, in one scheme, by a purely rotational term. The quantities of interest are the radial behavior $r=\sqrt{g^2+h^2}$ and the angular winding of trajectories, which together indicate whether the flow looks like convergence to a fixed point or exhibits sustained angular motion reminiscent of a limit cycle.

The toy exists to stress-test the notion of universality by showing that global RG portraits are not invariant under all legitimate reparameterizations. While universal information is encoded in local behavior near fixed points, additional scheme-dependent terms can alter trajectories away from that regime without changing the underlying short-distance content. Concretely, the core flow $$\dot g=-g^3+bgh,\quad \dot h=-h^3-bgh$$ is modified in one scheme by a radius-preserving rotation $(\dot g,\dot h)\to(\dot g-S h,\dot h+S g)$, which can generate substantial angular winding even as the radius decreases. This is framed as a limitation: the toy illustrates how fixed-point versus cycle-like interpretations can depend on scheme choices rather than reflecting distinct physical phases.

To interpret the JSON results, compare the per-scheme diagnostics in `local_observables`, especially `winding_number`, `r_final`, and the resulting `classification`. In the provided data, both schemes yield “flow_like” behavior for all tested initial conditions, with decreasing radius and small total winding, so the summary reports no classification disagreements. The relevant trend is that scheme B introduces noticeably larger angular motion (nonzero winding) while preserving overall radial decay, highlighting how qualitative impressions of the flow can shift without changing endpoint behavior. What should not be over-interpreted is the absence of disagreements here as a general statement: the numerical values and classifications depend on integration time, thresholds, and the specific rotational term, and they serve only to connect the observed windings and radii back to the underlying flow equations, not to assert a physical prediction about real RG cycles.

---

## Toy 095 — Microcausality Leakage from UV Softening

This toy computes a proxy for the Pauli–Jordan commutator of a scalar field in 1+1 dimensions when the ultraviolet (UV) behavior is modified. The physical setup is a momentum-space integral with a modified dispersion relation $\omega(k)=\sqrt{k^2+m^2}\,(1+\alpha k^2/\Lambda^2)$, evaluated with a symmetric momentum cutoff $K$, and the quantity of interest is the spacetime kernel $\Delta(t,x)$ that diagnoses causal support. In a strictly local relativistic theory (with $c=1$), this kernel is supported inside the light cone $|x|\le t$, so any nonzero signal outside that region indicates a departure from microcausality.

The toy exists to expose a limitation: UV “softening” mechanisms intended to tame short-distance behavior can trade those improvements for violations of locality. By altering high-momentum phase and group velocities, the modified dispersion changes how information propagates, and when combined with a finite cutoff it can produce commutator support outside the light cone even though the modification looks mild. This is framed as a stress test rather than a prediction, illustrating that locality is a boundary condition on UV completion and that different regularizations or form factors can qualitatively change causal structure without implying a physically acceptable theory.

To interpret the JSON results, compare the maxima of $|\Delta(t,x)|$ inside and outside the light cone and focus on the `leakage_ratio`, defined as their ratio. Values near or above unity indicate that outside-lightcone contributions are comparable to or larger than the interior signal, with trends depending on $\alpha$, $\Lambda$, and $t$; in the provided data the largest ratios occur at moderate $\alpha$ and intermediate $\Lambda$. What should not be over-interpreted are the absolute magnitudes, which are small and sensitive to discretization and the finite cutoff $K$; instead, the meaningful information is the relative scaling captured by the ratio and how it reflects the way the modified $\omega(k)$ reshapes the integral defining $\Delta(t,x)$.

---

## Toy 096 — Positivity bounds as a UV-completability filter

This toy scans a simple effective-field-theory (EFT) proxy for forward elastic scattering, where the low-energy forward amplitude is truncated to $A(s)=c_2 s^2+c_4 s^4+c_6 s^6$ with nonnegative coefficients $(c_2,c_4,c_6)$ on a fixed grid. The physical setup being abstracted is a causal, unitary, analytic UV completion whose forward-limit dispersion relation constrains how rapidly the amplitude can curve as a function of energy; the quantity of interest is therefore not a cross section, but whether a chosen set of Wilson-like coefficients could plausibly sit inside the positivity/analyticity-allowed region.

The toy exists to illustrate a failure mode of “generic EFT coefficient choices”: even if coefficients look innocuous and respect basic sign constraints, analyticity-driven positivity bounds can still exclude them. Concretely, besides $c_2\ge 0$, $c_4\ge 0$, and $c_6\ge 0$, it imposes a convexity-style dispersion constraint $c_4^2\le \beta\,c_2 c_6$ (with $\beta$ controlling how strong the assumed UV input is), which geometrically carves coefficient space into a narrow cone rather than a large volume. The point is a stress test of UV-completability assumptions—demonstrating how small deformations in coefficients can push you outside the allowed cone—not a prediction that any particular $(c_2,c_4,c_6)$ will be realized in nature.

To interpret the JSON, start with `observables.summary`: `fractions.allowed` is the fraction of scanned grid points that satisfy all bounds (here 0.648), while `fractions.strictly_inside` (0.544) and `fractions.boundary` (0.104) separate interior points from those saturating the dispersion inequality. In each entry of `sample_points`, the key diagnostics are `local_observables.dispersion_lhs` (which is $c_4^2$) and `local_observables.dispersion_rhs` (which is $\beta c_2 c_6$), and the `classification` tells you whether the inequality is violated, saturated, or strictly satisfied; trends to watch are how often large $c_4$ at small $c_2 c_6$ triggers `violates_dispersion_bound`, and how frequently saturation appears along “edges” like $c_4=0$ or $c_2 c_6$ just large enough to match $c_4^2$. What should not be over-interpreted are the absolute fractions (they depend on grid choice, $\beta$, and the tolerance `eps`) or any notion that “boundary points” are physically special beyond representing $c_4^2\approx \beta c_2 c_6$ in this toy; the JSON values are best read as a bookkeeping map from the amplitude expansion $A(s)$ to the inequality comparing $c_4^2$ against $\beta c_2 c_6$, indicating which coefficient combinations pass the analyticity/positivity filter.

---

## Toy 097 — Non-unique global reconstructions from identical local marginals

This toy works with a three-qubit system $(A,B,C)$ and compares two distinct global density matrices $\rho_{ABC}$: a pure GHZ state $|\mathrm{GHZ}\rangle=(|000\rangle+|111\rangle)/\sqrt{2}$ and a classical mixture $\rho_{\mathrm{mix}}=\tfrac12|000\rangle\langle000|+\tfrac12|111\rangle\langle111|$. The physical setup being abstracted is “reconstruct the whole from overlapping parts”: you are given the same two-body reduced states on $AB$ and $BC$ (and likewise on $AC$), and the quantity of interest is whether that local information uniquely determines the global correlations, entanglement, and purity.

The toy exists to expose a conceptual boundary: local marginals can fail to fix a unique global state, and in operator-algebra language the very meaning of a “reduced state” can depend on the chosen local algebra (e.g., whether it has a center/edge-mode sector). Here, the failure mode is concrete: $\rho_{ABC}$ can be pure and highly entangled or mixed and purely classical while sharing identical overlapping marginals, so any reconstruction must implicitly choose extra structure beyond the data. The stress test is implemented by contrasting two deterministic reconstructions from the same inputs—(i) a Jaynes-style maximum-entropy completion $\rho_{\mathrm{ME}}=\arg\max_\rho S(\rho)$ subject to fixed $\rho_{AB}$ and $\rho_{BC}$, and (ii) a “center-extended” completion that treats $B$ as carrying an explicit classical label and builds a block-diagonal global state—demonstrating that different algebra/prior choices pick different completions without changing the supplied marginals.

To interpret the JSON, use `notes.marginal_checks` to confirm the setup: the norms `||rho_AB(GHZ)-rho_AB(mix)||` and `||rho_BC(GHZ)-rho_BC(mix)||` are $\sim 10^{-16}$, meaning the local data are numerically identical. Then compare the four `sample_points` via `local_observables`: the key discriminants are `purity_Tr_rho2` (near 1 for `true_state_ghz_pure`, but 0.5 for the mixture-like states) and `fidelity_with_GHZ_projector` (near 1 for GHZ, near 0.5 for mixture-like completions), while `I3_tripartite_mutual_info` separates “genuinely global” structure (approximately 0 for GHZ here) from the classical-mixture pattern (approximately $\ln 2$). What should not be over-interpreted is the precise numerical closeness between `reconstruction_max_entropy_from_AB_BC` and the mixture in this run (it reflects the particular IPF-like approximation, tolerances, and the fact that the max-entropy principle favors a higher-entropy completion), nor should one read $I_3$ or fidelity as universal order parameters; instead, the JSON should be read as illustrating that fixing $\rho_{AB}$ and $\rho_{BC}$ constrains but does not determine $\rho_{ABC}$, so changes in global purity, GHZ overlap, and $I_3$ are signals of reconstruction ambiguity rather than physical prediction.

---

## Toy 098 — Thermalization ambiguity: chaotic equilibration vs integrable constraints

This toy compares exact unitary time evolution in two length-$L$ spin-1/2 chains starting from the same simple product state (a Néel pattern), but with different Hamiltonians: an integrable transverse-field Ising model (no longitudinal field) and a nonintegrable “chaotic” variant with both transverse and longitudinal fields. The setup tracks how a fixed initial state $|\psi_0\rangle$ spreads under $|\psi(t)\rangle=e^{-iHt}|\psi_0\rangle$, and the quantities of interest are operational indicators of relaxation: the Loschmidt echo $L(t)=|\langle\psi_0|\psi(t)\rangle|^2$, the chain-averaged magnetization $m_z(t)=(1/L)\sum_i\langle\sigma_i^z\rangle$, and the half-chain entanglement entropy $S_{\mathrm{half}}(t)$ computed from the reduced density matrix of the first $L/2$ spins.

The toy exists to demonstrate that “thermalization” is not a single yes/no property of unitary dynamics, but a boundary sensitive to dynamics class (integrable vs chaotic), which observable you watch, and how you coarse-grain in time and system size. Integrable models can look highly “thermal” in some channels (rapid entanglement growth) while remaining rigid in others (extra conserved quantities pin certain one-point functions), whereas chaotic models tend to suppress late-time fluctuations but can still show sizable recurrences in small systems. The stress test here is not about predicting real materials; it is about exposing that a late-time equilibration proxy based on variances can flip depending on whether you judge by $m_z(t)$ or by $S_{\mathrm{half}}(t)$, and on the chosen late-time window.

To interpret the JSON, read `observables.time_series` as the raw trajectories and `sample_points[*].local_observables` as late-time window summaries starting at `late_start_time` (here 5.2) out to `t_max` (8.0). For each model, smaller late-time variances `mz_var` and `S_half_var` indicate closer approach to a steady plateau for that observable, while `loschmidt_min` and `loschmidt_max` in the same window quantify recurrences through the range of $L(t)$. In this run, the chaotic model has much smaller `S_half_var` than the integrable one (more stable entanglement plateau), but the integrable model has an essentially zero `mz_var` (magnetization is nearly pinned at numerical precision), which is why `thermalization_proxy_supports_ETH` is false: the “ETH-like” signature depends on which observable you privilege. What should not be over-interpreted are absolute entropy levels or specific recurrence amplitudes (they are strongly finite-size and cutoff dependent), nor should one treat a low `mz_var` in the integrable case as evidence of thermalization; instead, connect the numbers back to the definitions: $L(t)$ flags reversibility/recurrence, $m_z(t)$ tests local one-point relaxation, and $S_{\mathrm{half}}(t)$ tracks entanglement spreading, and the JSON is illustrating that these can disagree even with identical initial energies and deterministic evolution.

---

## Toy 099 — QFT Boundary Dashboard

This toy ingests the JSON outputs of several boundary-focused quantum field theory toys (090–098) and consolidates them into a single diagnostic “dashboard.” Rather than simulating a physical system directly, it treats each upstream toy as probing a distinct stress point—such as analyticity, locality, unitarity, or reconstruction—and converts their reported diagnostics into normalized boundary severities between 0 and 1. The primary quantities of interest are the per-boundary severities and an overall control score defined conceptually as $C = 1 - \langle s_i \rangle$, where $s_i$ denotes the severity associated with each boundary signal included.

The purpose of this toy is to illustrate a meta-level limitation: even when individual diagnostics are well-defined, combining them into a coherent global assessment is itself nontrivial and heuristic. It exposes ambiguities in normalization, aggregation, and comparison across qualitatively different failure modes, framing the exercise as a stress test of reporting and closure rather than a physical prediction. The dashboard demonstrates how “what fails first” can depend on conventions and proxies, emphasizing that the ordering of severities reflects methodological pressure points rather than an absolute hierarchy of physical inconsistency.

The JSON results should be read as a structured summary of these pressures. Higher severity values (closer to 1) indicate stronger boundary pressure in the corresponding category, while the boundary_ordering array simply ranks the non-null severities from largest to smallest. The control_score_0_to_1 aggregates these into a single number but should not be over-interpreted as a measure of theoretical validity; it only reflects the mean of the included heuristics. Null entries indicate missing or non-triggered diagnostics, and small numerical differences should not be taken as precise quantitative statements. Conceptually, the reported severities map the qualitative outcomes of the underlying toys back onto the normalized scale used in the control score, closing the loop between individual boundary probes and the aggregate summary.

---

## Toy 100 — Dark Matter Origin: Mirror Remnants vs Horizon Radiation

This toy compares two speculative dark matter origin mechanisms using simple, deterministic proxies rather than a full cosmological simulation. One mechanism models a mirror-sector dark QCD that forms dense dark baryons which may gravitationally collapse into Planck-scale remnants, while the other models gravitational particle production from an inflationary horizon. The quantity of interest is a proxy relic abundance $\Omega_{\mathrm{proxy}}$, constructed from minimal scaling assumptions, such as $\Omega_{\mathrm{mirror}} \propto \eta_d[(1-f_c)m_B+f_c M_{\mathrm{rem}}]$ for mirror remnants and $\Omega_{\mathrm{horizon}} \propto m H^3 e^{-2\pi m/H}(1-e^{-3N})$ for horizon radiation, where the symbols denote the scanned parameters described in the text.

The toy exists to stress-test qualitative claims about “naturalness” and robustness of dark matter origin stories by translating them into explicit parameter dependences. Rather than predicting the actual dark matter abundance, it exposes how strongly each mechanism depends on tuning of asymmetries, mass ratios, or durations, and whether viable outcomes occupy broad or narrow regions of parameter space. The conceptual issue being illustrated is that mechanisms often described as generic may, under simple consistency proxies, fail to populate the target abundance except in finely adjusted corners, highlighting a limitation in qualitative reasoning rather than a failure of fundamental physics.

The JSON output should be interpreted as a grid scan over parameters for both mechanisms, with each sample point reporting a proxy abundance, its logarithm, and whether it lies within a specified tolerance of the target. The summary fractions report what portion of the scanned volume “matches” the target and are combined into a robustness score; in this run, both mechanisms yield zero matching fraction, indicating no viable region under the chosen proxies. The absolute numerical values of $\Omega_{\mathrm{proxy}}$ should not be over-interpreted, nor should the absence of matches be taken as exclusion of the ideas themselves. Instead, trends such as exponential suppression with $m/H$ or the extreme smallness of collapse fractions encode how the underlying equations drive sensitivity and illustrate where the toy’s stress test applies.

---

